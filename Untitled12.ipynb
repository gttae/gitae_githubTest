{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+oZBvggudWk8XXVU6vOXi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gttae/gitae_githubTest/blob/master/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FkyLKsLcowr"
      },
      "outputs": [],
      "source": [
        "import deepspeed\n",
        "import argparse\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "from allennlp.training.metrics import BLEU\n",
        "from itertools import cycle\n",
        "from pathlib import Path\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "def get_arguments():\n",
        "    parser = argparse.ArgumentParser(description='Train T5 on Lakh Midi Dataset Instruments-Lyrics')\n",
        "\n",
        "    parser.add_argument('--dataset-file', '-df', type=str, required=True,\n",
        "                        help='Dataset parquet file')\n",
        "\n",
        "    parser.add_argument('--vocabulary-prefix', '-v', type=str, default='',\n",
        "                        help='Prefix of the vocab files: <pref>_instrumental.vocab, <prf>_lyric.vocab')\n",
        "\n",
        "    parser.add_argument('--save-dir', '-sd', type=str, required=True,\n",
        "                        help='Directory to save checkpoints, states, event logs')\n",
        "\n",
        "    parser.add_argument('--train-split', '-ts', type=float, default=0.9,\n",
        "                        help='Percentage of the dataset to use for training')\n",
        "\n",
        "    parser.add_argument('--epochs', '-e', type=int, default=20,\n",
        "                        help='Number of epochs')\n",
        "\n",
        "    parser.add_argument('--validate-every', '-ve', type=int, default=200,\n",
        "                        help='Validate every n batches')\n",
        "\n",
        "    parser.add_argument('--generate-every', '-ge', type=int, default=400,\n",
        "                        help='Generate every n batches')\n",
        "\n",
        "    parser.add_argument('--print-training-loss-every', '-ptle', type=int, default=20,\n",
        "                        help='It will average training loss and print it every n steps')\n",
        "\n",
        "    parser.add_argument('--validate-size', '-vs', type=int, default=40,\n",
        "                        help='Will calculate average of validation loss for n batches')\n",
        "\n",
        "    parser.add_argument('--validate-batch-size', '-vss', type=int, default=1,\n",
        "                        help='Batch size for validation dataset')\n",
        "\n",
        "    parser.add_argument('--checkpoints-per-epoch', '-cpp', type=int, default=3,\n",
        "                        help='How many checkpoints to keep per epoch')\n",
        "\n",
        "    parser.add_argument('--local_rank', type=int, default=-1,\n",
        "                        help='Local rank passed from distributed launcher')\n",
        "\n",
        "    parser = deepspeed.add_config_arguments(parser)\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class MidiDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset_file, tokenizer, max_length):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        df = pd.read_parquet(dataset_file)\n",
        "        self.files = list(df['file'])\n",
        "        self.inputs = self.prepare_input_sequences(df['instrumental'])\n",
        "        self.targets = self.prepare_target_sequences(df['lyric'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.targets[idx], self.files[idx]\n",
        "\n",
        "    def prepare_input_sequences(self, sequences):\n",
        "        return [self.tokenize_sequence(seq) for seq in sequences]\n",
        "\n",
        "    def prepare_target_sequences(self, sequences):\n",
        "        return [self.tokenize_sequence(seq) for seq in sequences]\n",
        "\n",
        "    def tokenize_sequence(self, sequence):\n",
        "        inputs = f\"lyric generation: {sequence}\"\n",
        "        return self.tokenizer.encode_plus(inputs, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = get_arguments()\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "    dataset = MidiDataset(args.dataset_file, tokenizer, max_length=512)\n",
        "\n",
        "    train_size = int(args.train_split * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_log_dir = os.path.join(args.save_dir, 'train')\n",
        "    val_log_dir = os.path.join(args.save_dir, 'val')\n",
        "    Path(train_log_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(val_log_dir).mkdir(parents=True, exist_ok=True)\n",
        "    writer_train = SummaryWriter(log_dir=train_log_dir)\n",
        "    writer_val = SummaryWriter(log_dir=val_log_dir)\n",
        "\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
        "\n",
        "    model_engine, optimizer, trainloader, _ = deepspeed.initialize(args=args, model=model, model_parameters=model.parameters(),  training_data=train_dataset, collate_fn=collate_fn_zero_pad)\n",
        "    device = model_engine.local_rank\n",
        "\n",
        "    torch.manual_seed(torch.initial_seed())\n",
        "    val_loader_ = DataLoader(val_dataset, batch_size=args.validate_batch_size, shuffle=True, collate_fn=collate_fn_zero_pad)\n",
        "    val_loader = cycle(val_loader_)\n",
        "\n",
        "    num_batches = (len(train_dataset) + trainloader.batch_size - 1) // trainloader.batch_size\n",
        "\n",
        "    save_every = num_batches // args.checkpoints_per_epoch\n",
        "    save_at = 0\n",
        "    saving_steps = []\n",
        "    for _ in range(args.checkpoints_per_epoch - 1):\n",
        "        save_at += save_every\n",
        "        saving_steps.append(save_at)\n",
        "    saving_steps.append(num_batches - 1)\n",
        "\n",
        "    print(\"\\n\", \"Train Dataset - size: {}, batches: {}\".format(len(train_dataset), num_batches), \"\\n\")\n",
        "    print(\"\\n\", \"Validate Dataset - size: {}, batches: {}\".format(len(val_dataset), len(val_loader_)), \"\\n\")\n",
        "\n",
        "    checkpoint_name, client_state = model_engine.load_checkpoint(args.save_dir, load_module_strict=False)\n",
        "\n",
        "    if checkpoint_name is not None:\n",
        "        print(\"\\nLoaded checkpoint: {}\\n\".format(checkpoint_name))\n",
        "        i = client_state['i']\n",
        "        i += 1\n",
        "        epoch, step = divmod(i, num_batches)\n",
        "        print(\"Epoch: {}, step: {}, i: {}\".format(epoch, step, i))\n",
        "        if step == 0:\n",
        "            print(\"Starting next epoch...\")\n",
        "            rng = torch.get_rng_state()\n",
        "            trainloader = iter(trainloader)\n",
        "        else:\n",
        "            rng = torch.load(os.path.join(args.save_dir, 'rng_state.pt'))\n",
        "            torch.set_rng\n"
      ]
    }
  ]
}